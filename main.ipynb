{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f749bcce",
   "metadata": {},
   "source": [
    "## Exercise 1: Implementation of multi-layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0276322",
   "metadata": {},
   "source": [
    "#### 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce139d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089ac468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, W, B):\n",
    "        self.W = W\n",
    "        self.B = B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d3b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(K, D, D_i, D_o):\n",
    "    W = [None] * (K+1)\n",
    "    B = [None] * (K+1)\n",
    "\n",
    "    #initialize first layer\n",
    "    W[0] = np.random.normal(size=(D, D_i))\n",
    "    B[0] = np.random.normal(size =(D,1))\n",
    "\n",
    "    #hidden layers\n",
    "    for layer in range(1,K):\n",
    "        W[layer] = np.random.normal(0, np.sqrt(2 / D), size=(D, D)) #He initialization\n",
    "        B[layer] = np.zeros((D,1))\n",
    "\n",
    "    #last layer\n",
    "    W[-1] = np.random.normal(size=(D_o, D))\n",
    "    B[-1]= np.random.normal(size =(D_o,1))\n",
    "\n",
    "\n",
    "    return W,B\n",
    "\n",
    "# Number of hidden layers\n",
    "K = 2\n",
    "# Number of neurons per layer\n",
    "D = 6\n",
    "# Input layer dimension\n",
    "D_i = 784\n",
    "# Output layer dimension\n",
    "D_o = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94d7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, B = initialize(K, D, D_i, D_o)\n",
    "model = Model(W, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f1004",
   "metadata": {},
   "source": [
    "#### 2. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ea7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    activation = x.clip(0.0)\n",
    "    return activation\n",
    "\n",
    "def relu_der(x):\n",
    "    dx = np.array(x)\n",
    "    dx[dx>0] = 1\n",
    "    dx[dx<=0] = 0\n",
    "    return dx\n",
    "\n",
    "def sigmoid(x):\n",
    "    activation = 1.0 / (1.0 + np.exp(-x))\n",
    "    return activation\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    sigma = 1.0 / (1.0 + np.exp(-x))\n",
    "    dx = sigma * (1.0 - sigma)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14844dab",
   "metadata": {},
   "source": [
    "#### 3. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32a97ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net_input, W, B):\n",
    " \n",
    "    K = len(W) -1 # number of layers\n",
    "\n",
    "    F = [None] * (K+1) # pre-activations at each layer\n",
    "    H = [None] * (K+1) # activations\n",
    "\n",
    "    H[0] = net_input\n",
    "\n",
    "    print(f\"F init len: {len(F)}\")\n",
    "    print(f\"H init len: {len(H)}\")\n",
    "    print(f\"Input shape: {net_input.shape}\")\n",
    "    # Run through the layers, calculating F[0...K-1] and H[1...K]\n",
    "    for layer in range(K):\n",
    "\n",
    "        print(f\"Processing layer {layer}/{K}\")\n",
    "        print(f\"W[{layer}] shape = {W[layer].shape}\")\n",
    "        print(f\"B[{layer}] shape = {B[layer].shape}\")\n",
    "\n",
    "\n",
    "        F[layer] = B[layer] + W[layer] @ H[layer] \n",
    "        print(f\"F[{layer}] shape = {F[layer].shape}\")\n",
    "\n",
    "\n",
    "        H[layer+1] = relu(F[layer])\n",
    "        print(f\"H[{layer+1}] shape = {H[layer].shape}\")\n",
    "\n",
    "    \n",
    "    F[K] = B[K] + W[K] @ H[K] \n",
    "\n",
    "    net_output = F[K]\n",
    "    print(f\"net_output shape = {net_output.shape}\")\n",
    "\n",
    "\n",
    "    return net_output, F, H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52773c",
   "metadata": {},
   "source": [
    "#### 4. Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4348c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\n",
    "    shifted_x = x - np.max(x, axis=-1, keepdims=True) # For numerical stability when f >> 0,preventing overflow of exp(x)\n",
    "    \n",
    "    exp_x = np.exp(shifted_x)\n",
    "    probs = exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def cross_entropy_cost(net_output, y):\n",
    "    I = y.shape[1]  # Number of data points if data points as columns\n",
    "    probs = softmax(net_output)\n",
    "    return np.sum(-1 * y * np.log(probs))/I\n",
    "\n",
    "def d_cost_d_output(net_output, y):\n",
    "    I = y.shape[1] # Number of data points if data points as columns\n",
    "    probs = softmax(net_output)\n",
    "    print(f\"probs shape: {probs.shape}\")\n",
    "    out = 2*np.sum(probs - y)/I\n",
    "    print(f\"d_cost_d_output shape: {out.shape}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856cf23",
   "metadata": {},
   "source": [
    "#### 5. Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7cefb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(W, B, F, H, y):\n",
    "\n",
    "    K = len(W)-1\n",
    "    # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n",
    "    dl_dW = [None] * (K+1)\n",
    "    dl_dB = [None] * (K+1)\n",
    "    # And we'll store the derivatives of the cost wrt the activation and preactivations in lists\n",
    "    dl_df = [None] * (K+1)\n",
    "    dl_dh = [None] * (K+1)\n",
    "    # Again for convenience we'll stick with the convention that H[0] is the net input and F[k] in the net output\n",
    "\n",
    "    print(\"Backprop: ------------------------------\")\n",
    "    # Compute derivatives of the cost wrt the network output\n",
    "    dl_df[K] = np.array(d_cost_d_output(F[K].T,y))\n",
    "    print(f\"dl_df[{K}] shape: {dl_df[K].shape}\")\n",
    "\n",
    "    # Change the range to the correct order in which the layers should be procesed.\n",
    "    layer_range = np.flip(range(K+1))\n",
    "\n",
    "    for layer in layer_range:\n",
    "        # Calculate the derivatives of the cost wrt the biases at layer from dl_df[layer].\n",
    "        dl_dB[layer] = np.sum(dl_df[layer], axis=1, keepdims=True) \n",
    "        print(f\"dl_dB[{layer}] shape: {dl_dB[layer].shape}\")\n",
    "\n",
    "\n",
    "        # Calculate the derivatives of the cost wrt the weights at layer from dl_df[layer] and H[layer] \n",
    "        dl_dW[layer] = dl_df[layer] @ H[layer].T\n",
    "        print(f\"dl_dW[{layer}] shape: {dl_dW[layer].shape}\")\n",
    "\n",
    "\n",
    "        # Calculate the derivatives of cost wrt activations from weight and derivatives of next preactivations \n",
    "        dl_dh[layer] =  W[layer].T @ dl_df[layer]\n",
    "        print(f\"dl_dh[{layer}] shape: {dl_dh[layer].shape}\")\n",
    "\n",
    "\n",
    "        if layer > 0:\n",
    "            # Calculate derivatives of the cost wrt pre-activation f \n",
    "            dl_df[layer-1] = relu_der(F[layer-1]) * dl_dh[layer] \n",
    "            print(f\"dl_df[{layer-1}] shape: {dl_df[layer-1].shape}\")\n",
    "\n",
    "            \n",
    "    return dl_dW, dl_dB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6192c91",
   "metadata": {},
   "source": [
    "#### 6. Step & parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f413cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W, B, dW, dB, lr):\n",
    "    W_new = W - lr*dW\n",
    "    B_new = B - lr*dB\n",
    "    return W_new, B_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b9304",
   "metadata": {},
   "source": [
    "#### 7. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920c7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x, y):\n",
    "    res_logits = forward_pass(x, model.W, model.B)\n",
    "    res = softmax(res_logits)\n",
    "\n",
    "    # binarize\n",
    "    res_binary = np.where(res == np.max(res), 1,0)\n",
    "    acc = accuracy(y, res_binary)\n",
    "\n",
    "    cost = cross_entropy_cost(res_logits, y)\n",
    "    \n",
    "    return res, acc, cost\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # having array of 1x10 vectors for true and pred, binary\n",
    "    y_true_ind = np.argmax(y_true, axis=1)\n",
    "    y_pred_ind = np.argmax(y_pred, axis=1)\n",
    "    acc = np.mean(y_true_ind == y_pred_ind)\n",
    "\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3adca9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(y, preds):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca75c13",
   "metadata": {},
   "source": [
    "#### 8. Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ffc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(x,y, size):\n",
    "    assert len(x) == len(y)\n",
    "    p = np.random.permutation(len(x))\n",
    "    # x[p], y[p] are permuted arrays\n",
    "\n",
    "    num_batches = math.ceil(len(x)/size)\n",
    "\n",
    "\n",
    "    x_batched = np.array_split(x[p], num_batches)\n",
    "    y_batched = np.array_split(y[p], num_batches)\n",
    "\n",
    "    return x_batched, y_batched\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c7e6e",
   "metadata": {},
   "source": [
    "#### 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "107eed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "\n",
    "# test = imageio.imread(\"MNIST/Train/4/0001.png\")\n",
    "# test = np.array(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "867986db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_mnist\n",
    "X_train, Y_train, X_test, Y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7780051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_test, Y_test, model, epochs, lr, batch_size):\n",
    "\n",
    "    X_batches, Y_batches = random_mini_batches(X_train, Y_train, batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for X_batch, Y_batch in zip(X_batches, Y_batches):\n",
    "            # Forward pass\n",
    "            net_out, F, H = forward_pass(X_batch.T, model.W, model.B)\n",
    "\n",
    "            # Loss calculation\n",
    "            loss = cross_entropy_cost(net_out, Y_batch.T)\n",
    "\n",
    "            # Backward pass\n",
    "            dl_dW, dl_dB = backward_pass(model.W, model.B, F, H, Y_batch)\n",
    "\n",
    "            # Update parameters layer by layer\n",
    "            for k in range(len(model.W)):\n",
    "                model.W[k], model.B[k] = update_parameters(\n",
    "                    model.W[k], model.B[k],\n",
    "                    dl_dW[k], dl_dB[k],\n",
    "                    lr\n",
    "                )\n",
    "\n",
    "        # Evaluate after each epoch\n",
    "        _, train_acc, train_cost = predict(model, X_train.T, Y_train.T)\n",
    "        _, test_acc, test_cost = predict(model, X_test.T, Y_test.T)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \"\n",
    "              f\"| Train acc: {train_acc:.4f} \"\n",
    "              f\"| Test acc: {test_acc:.4f} \"\n",
    "              f\"| Train cost: {train_cost:.4f}\"\n",
    "              f\"| Test cost: {test_cost:.4f}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f22c113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F init len: 3\n",
      "H init len: 3\n",
      "Input shape: (784, 32)\n",
      "Processing layer 0/2\n",
      "W[0] shape = (6, 784)\n",
      "B[0] shape = (6, 1)\n",
      "F[0] shape = (6, 32)\n",
      "H[1] shape = (784, 32)\n",
      "Processing layer 1/2\n",
      "W[1] shape = (6, 6)\n",
      "B[1] shape = (6, 1)\n",
      "F[1] shape = (6, 32)\n",
      "H[2] shape = (6, 32)\n",
      "net_output shape = (10, 32)\n",
      "Backprop: ------------------------------\n",
      "dl_df[2] shape: ()\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAxisError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(X_train, Y_train, X_test, Y_test, model, epochs, lr, batch_size)\u001b[39m\n\u001b[32m     12\u001b[39m loss = cross_entropy_cost(net_out, Y_batch.T)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m dl_dW, dl_dB = \u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Update parameters layer by layer\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model.W)):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mbackward_pass\u001b[39m\u001b[34m(W, B, F, H, y)\u001b[39m\n\u001b[32m     18\u001b[39m layer_range = np.flip(\u001b[38;5;28mrange\u001b[39m(K+\u001b[32m1\u001b[39m))\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layer_range:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Calculate the derivatives of the cost wrt the biases at layer from dl_df[layer].\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     dl_dB[layer] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdl_dB[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdl_dB[layer].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Calculate the derivatives of the cost wrt the weights at layer from dl_df[layer] and H[layer] \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UNI 2025/Deep Learning/Handin_1/.venv/lib64/python3.12/site-packages/numpy/_core/fromnumeric.py:2425\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, _gentype):\n\u001b[32m   2418\u001b[39m     \u001b[38;5;66;03m# 2018-02-25, 1.15.0\u001b[39;00m\n\u001b[32m   2419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   2420\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCalling np.sum(generator) is deprecated.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2421\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse np.sum(np.fromiter(generator)) or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2422\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe python sum builtin instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2423\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2425\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2426\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n\u001b[32m   2428\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UNI 2025/Deep Learning/Handin_1/.venv/lib64/python3.12/site-packages/numpy/_core/fromnumeric.py:83\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAxisError\u001b[39m: axis 1 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "train_model(X_train, Y_train, X_test, Y_test, model, 5, 0.001, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f317d2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
